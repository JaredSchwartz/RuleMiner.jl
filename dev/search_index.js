var documenterSearchIndex = {"docs":
[{"location":"frequent_itemsets/#Frequent-Itemset-Mining","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"(Image: Diagram showing maximal itemsets as a subset of closed itemsets which are a subset of frequent itemsets)","category":"page"},{"location":"frequent_itemsets/#Description","page":"Frequent Itemset Mining","title":"Description","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"Frequent itemset mining is a fundamental technique in data mining focused on discovering itemsets that appear frequently in a transactional dataset. A frequent itemset is a set of items that occurs together in the data with a frequency no less than a specified minimum support threshold. Frequent itemsets form the basis of various data mining tasks, including association rule mining, sequential pattern mining, and correlation analysis.","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"The one caveat with frequent itemset mining is that depending on the support parameter and the structure of the data, these mining techniques can yield large numbers of patterns, especially in dense datasets or with low support thresholds. This challenge has led to the development of more concise representations like closed and maximal itemset mining.","category":"page"},{"location":"frequent_itemsets/#Formal-Definition","page":"Frequent Itemset Mining","title":"Formal Definition","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"Let:","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"I be the set of all items in the dataset\nX be an itemset, where X subseteq I\nD be the set of all transactions in the dataset\nsigma(X) be the support of itemset X in D\nsigma_min be the minimum support threshold","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"Then, an itemset X is a maximal frequent itemset if and only if: 1.\tThe support of X is greater than or equal to the minimum support threshold: ","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"sigma(X) geq sigma_min","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"Thus, FI, the set of all maximal frequent itemsets in I can be expressed as:","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"FI = X mid X subseteq I wedge sigma(X) geq sigma_min","category":"page"},{"location":"frequent_itemsets/#Algorithms","page":"Frequent Itemset Mining","title":"Algorithms","text":"","category":"section"},{"location":"frequent_itemsets/#ECLAT","page":"Frequent Itemset Mining","title":"ECLAT","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"The eclat function implements the [E]quivalence [CLA]ss [T]ransformation algorithm for frequent itemset mining proposed by Mohammad Zaki in 2000. This algorithm identifies frequent itemsets in a dataset utilizing a column-first search and supplied minimum support.","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"eclat(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"frequent_itemsets/#RuleMiner.eclat-Tuple{Transactions, Union{Float64, Int64}}","page":"Frequent Itemset Mining","title":"RuleMiner.eclat","text":"eclat(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nPerform frequent itemset mining using the ECLAT (Equivalence CLAss Transformation) algorithm  on a transactional dataset.\n\nECLAT is an efficient algorithm for discovering frequent itemsets, which are sets of items  that frequently occur together in the dataset.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nA DataFrame containing the discovered frequent itemsets with the following columns:\n\nItemset: Vector of item names in the frequent itemset.\nSupport: Relative support of the itemset.\nN: Absolute support count of the itemset.\nLength: Number of items in the itemset.\n\nAlgorithm Description\n\nThe ECLAT algorithm uses a depth-first search strategy and a vertical database layout to  efficiently mine frequent itemsets. It starts by computing the support of individual items,  sorts them in descending order of frequency, and then recursively builds larger itemsets. ECLAT's depth-first approach enables it to quickly identify long frequent itemsets, and it is most efficient for sparse datasets\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ' ')\n\n# Find frequent itemsets with 5% minimum support\nresult = eclat(txns, 0.05)\n\n# Find frequent itemsets with minimum 5,000 transactions\nresult = eclat(txns, 5_000)\n\nReferences\n\nZaki, Mohammed. “Scalable Algorithms for Association Mining.” Knowledge and Data Engineering, IEEE Transactions On 12 (June 1, 2000): 372–90. https://doi.org/10.1109/69.846291.\n\n\n\n\n\n","category":"method"},{"location":"frequent_itemsets/#FP-Growth","page":"Frequent Itemset Mining","title":"FP-Growth","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"The fpgrowth function implements the FP-Growth ([F]requent [P]attern Growth) algorithm for mining frequent itemsets. This algorithm, proposed by Han et al. in 2000, is an efficient method for discovering frequent itemsets in a dataset without candidate generation. It is generally more efficient than other algorithms when datasets are dense, as the internal FP tree data structure it builds efficiently summarizes the relationships and supports of the itemsets.","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"fpgrowth(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"frequent_itemsets/#RuleMiner.fpgrowth-Tuple{Transactions, Union{Float64, Int64}}","page":"Frequent Itemset Mining","title":"RuleMiner.fpgrowth","text":"fpgrowth(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify frequent itemsets in a transactional dataset with the FPGrowth algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nThe FPGrowth algorithm is a mining technique that builds a compact summary of the transction data called an FP tree.  This tree structure summarizes the supports and relationships between items in a way that can be easily transversed and processed to find frequent itemsets.  FPGrowth is particularly efficient for datasets with long transactions or sparse frequent itemsets.\n\nFP-tree Construction: Builds a compact representation of the dataset, organizing items  by their frequency to allow efficient mining.\nRecursive Tree Traversal: \nProcesses itemsets from least frequent to most frequent.\nFor each item, creates a conditional FP-tree and recursively mines it.\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ' ')\n\n# Find frequent itemsets with 5% minimum support\nresult = fpgrowth(txns, 0.05)\n\n# Find frequent itemsets with minimum 5,000 transactions\nresult = fpgrowth(txns, 5_000)\n\nReferences\n\nHan, Jiawei, Jian Pei, and Yiwen Yin. “Mining Frequent Patterns without Candidate Generation.” SIGMOD Rec. 29, no. 2 (May 16, 2000): 1–12. https://doi.org/10.1145/335191.335372.\n\n\n\n\n\n","category":"method"},{"location":"association_rules/#Association-Rule-Mining","page":"Association Rule Mining","title":"Association Rule Mining","text":"","category":"section"},{"location":"association_rules/#Description","page":"Association Rule Mining","title":"Description","text":"","category":"section"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Association rule mining is a fundamental technique in data mining and machine learning that aims to uncover interesting relationships, correlations, or patterns within large datasets. Originally developed for market basket analysis in retail, it has since found applications in various fields such as web usage mining, intrusion detection, and bioinformatics. The primary goal of association rule mining is to identify strong association rules rules discovered in databases using different measures of interestingness like support, confidence, coverage, and lift.","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"At its core, association rule mining works by examining frequent if-then patterns in transactional databases. These patterns, known as association rules, take the form \"if A, then B,\" where A and B are sets of items. For example, in a supermarket context, a rule might be \"if a customer buys bread and butter, they are likely to buy milk.\" The strength of these rules is typically measured by support (how frequently the items appear together), confidence (how often the rule is found to be true), converage(how often B ocurrs in in the databse with or without A), and lift (the ratio of observed support to expected support if A and B were independent). By filtering association rules with these metrics, analysts can filter out weak or uninteresting rules and focus on those that are most likely to provide valuable insights or actionable information.","category":"page"},{"location":"association_rules/#Formal-Definition","page":"Association Rule Mining","title":"Formal Definition","text":"","category":"section"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Let:","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"I = i_1 i_2  i_n be the set of all items in the dataset\nD = T_1 T_2  T_m be the set of all transactions, where each T_j subseteq I\nA B subseteq I and A cap B = emptyset","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"An association rule is an implication of the form A Rightarrow B, where:","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"A is called the antecedent (or left-hand side)\nB is called the consequent (or right-hand side)","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"For a given rule A Rightarrow B, these measures are defined:","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Support: sigma(A Rightarrow B) = fracT_j in D  A cup B subseteq T_jD\nConfidence: chi(A Rightarrow B) = fracsigma(A cup B)sigma(A)\nCoverage: gamma(A Rightarrow B) = sigma(B) = fracT_j in D  B subseteq T_jD\nLift: L(A Rightarrow B) = fracsigma(A cup B)sigma(A) cdot sigma(B)","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Let sigma_min and chi_min be user-defined minimum thresholds for support and confidence, respectively.","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Then, the set of all valid association rules (AR) can be defined as:","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"AR = (A Rightarrow B) mid A B subseteq I newline\nwedge  A cap B = emptyset \nwedge sigma(A Rightarrow B) geq sigma_min \nwedge  chi(A Rightarrow B) geq chi_min","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"The process of association rule mining involves:","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Finding all frequent itemsets F = Z subseteq I mid sigma(Z) geq sigma_min\nFor each frequent itemset Z in F, generate all non-empty subsets A subset Z\nFor each such subset A, form the rule A Rightarrow (Z setminus A) if chi(A Rightarrow (Z setminus A)) geq chi_min","category":"page"},{"location":"association_rules/#Algorithms","page":"Association Rule Mining","title":"Algorithms","text":"","category":"section"},{"location":"association_rules/#A-Priori","page":"Association Rule Mining","title":"A Priori","text":"","category":"section"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"The apriori function implements the A Priori algorithm for association rule mining first proposed by Rakesh Agrawal and Srikant Ramakrishnan in 1994. This algorithm identifies frequent itemsets in a dataset and generates association rules based on specified support thresholds.","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"apriori(txns::Transactions, min_support::Union{Int,Float64}, max_length::Int)","category":"page"},{"location":"association_rules/#RuleMiner.apriori-Tuple{Transactions, Union{Float64, Int64}, Int64}","page":"Association Rule Mining","title":"RuleMiner.apriori","text":"apriori(txns::Transactions, min_support::Union{Int,Float64}, max_length::Int)::DataFrame\n\nIdentify association rules in a transactional dataset using the A Priori Algorithm\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\nmax_length::Int: The maximum length of the rules to be generated.\n\nReturns\n\nA DataFrame containing the discovered association rules with the following columns:\n\nLHS: The left-hand side (antecedent) of the rule.\nRHS: The right-hand side (consequent) of the rule.\nSupport: Relative support of the rule.\nConfidence: Confidence of the rule.\nCoverage: Coverage (RHS support) of the rule.\nLift: Lift of the association rule.\nN: Absolute support of the association rule.\nLength: The number of items in the association rule.\n\nDescription\n\nThe Apriori algorithm employs a breadth-first, level-wise search strategy to discover  frequent itemsets. It starts by identifying frequent individual items and iteratively  builds larger itemsets by combining smaller frequent itemsets. At each iteration, it  generates candidate itemsets of size k from itemsets of size k-1, then prunes candidates  that have any infrequent subset. \n\nThe algorithm uses the downward closure property, which states that any subset of a frequent itemset must also be frequent. This is the defining pruning technique of A Priori. Once all frequent itemsets up to the specified maximum length are found, the algorithm generates association rules and  calculates their support, confidence, and other metrics.\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ' ')\n\n# Find rules with 5% min support and max length of 3\nresult = apriori(txns, 0.05, 3)\n\n# Find rules with with at least 5,000 instances and max length of 3\nresult = apriori(txns, 5_000, 3)\n\nReferences\n\nAgrawal, Rakesh, and Ramakrishnan Srikant. “Fast Algorithms for Mining Association Rules in Large Databases.” In Proceedings of the 20th International Conference on Very Large Data Bases, 487–99. VLDB ’94. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1994.\n\n\n\n\n\n","category":"method"},{"location":"maximal_itemsets/#Maximal-Itemset-Mining","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"(Image: Diagram showing maximal itemsets as a subset of closed itemsets which are a subset of frequent itemsets)","category":"page"},{"location":"maximal_itemsets/#Description","page":"Maximal Itemset Mining","title":"Description","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"Maximal itemset mining is a set of techniques focused on discovering maximal itemsets in a transactional dataset. A maximal itemset is one which appears frequently in the data (above the minimum support threshold) and which is not a subset of any other frequent itemset. ","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"In other words, maximal itemsets are the largest possible combinations in the dataset of the items that meet a specified frequency threshold. They are a subset of closed itemsets, which in turn are a subset of all frequent itemsets.","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"The key advantage of mining maximal itemsets is its compact representation of all frequent patterns in the data. By identifying only the maximal frequent itemsets, the number of patterns generated is significantly reduced compared to frequent itemset mining. This approach is particularly valuable when dealing with high-dimensional data or datasets with long transactions.","category":"page"},{"location":"maximal_itemsets/#Formal-Definition","page":"Maximal Itemset Mining","title":"Formal Definition","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"Let:","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"I be the set of all items in the dataset\nX be an itemset, where X subseteq I\nD be the set of all transactions in the dataset\nsigma(X) be the support of itemset X in D\nsigma_min be the minimum support threshold","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"Then, an itemset X is a maximal frequent itemset if and only if: 1.\tThe support of X is greater than or equal to the minimum support threshold: ","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"sigma(X) geq sigma_min","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"2.\tThere does not exist a superset Y of X such that Y is also frequent: ","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"nexists Y supset X  sigma(Y) geq sigma_min","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"Thus, MFI, the set of all maximal frequent itemsets in I can be expressed as:","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"MFI = X mid X subseteq I wedge sigma(X) geq sigma_min wedge nexists Y supset X  sigma(Y) geq sigma_min","category":"page"},{"location":"maximal_itemsets/#Frequent-Itemset-Recovery","page":"Maximal Itemset Mining","title":"Frequent Itemset Recovery","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"Maximal itemsets can be used to recover all frequent itemsets by generating combinations from the mined itemset. However, unlike with closed itemsets, recovering the support of the frequent combinations is not possible.","category":"page"},{"location":"maximal_itemsets/#Algorithms","page":"Maximal Itemset Mining","title":"Algorithms","text":"","category":"section"},{"location":"maximal_itemsets/#FPMax","page":"Maximal Itemset Mining","title":"FPMax","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"The fpmax function implements the FPMax ([F]requent [P]attern Max) algorithm for mining closed itemsets. This algorithm, proposed by Gösta Grahne and Jianfei Zhu in 2005, builds on the FP-Growth alogrithm by mining FP trees to discover maximal itemsets in a dataset. It inherits many of the advantages of FP-Growth when it comes to dense datasets.","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"fpmax(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"maximal_itemsets/#RuleMiner.fpmax-Tuple{Transactions, Union{Float64, Int64}}","page":"Maximal Itemset Mining","title":"RuleMiner.fpmax","text":"fpmax(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify maximal frequent itemsets in a transactional dataset with the FPMax algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nThe FPMax algorithm is an extension of FP-Growth with  additional pruning techniques to focus on mining maximal itemsets. The algorithm operates in three main phases:\n\nFP-tree Construction: Builds a compact representation of the dataset, organizing items  by their frequency to allow efficient mining.\nRecursive Tree Traversal: \nProcesses itemsets from least frequent to most frequent.\nFor each item, creates a conditional FP-tree and recursively mines it.\nUses a depth-first search strategy, exploring longer itemsets before shorter ones.\nEmploys pruning techniques to avoid generating non-maximal itemsets.\nAdds an itemset to the candidate set when no frequent superset exists.\nMaximality Checking: After the recursive traversal, filters the candidate set to ensure  only truly maximal itemsets are included in the final output.\n\nFPMax is particularly efficient for datasets with long transactions or sparse frequent itemsets,  as it can significantly reduce the number of generated itemsets compared to algorithms that  find all frequent itemsets.\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ' ')\n\n# Find maximal frequent itemsets with 5% minimum support\nresult = fpmax(txns, 0.05)\n\n# Find maximal frequent itemsets with minimum 5,000 transactions\nresult = fpmax(txns, 5_000)\n\nReferences\n\nGrahne, Gösta, and Jianfei Zhu. “Fast Algorithms for Frequent Itemset Mining Using FP-Trees.” IEEE Transactions on Knowledge and Data Engineering 17, no. 10 (October 2005): 1347–62. https://doi.org/10.1109/TKDE.2005.166.\n\n\n\n\n\n","category":"method"},{"location":"maximal_itemsets/#GenMax","page":"Maximal Itemset Mining","title":"GenMax","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"The genmax function implements the GenMax algorithm for mining closed itemsets. This algorithm, proposed by Karam Gouda and Mohammad Zaki in 2005, utilizes a technique called progressive focusing to reduce the search space for maximal itemset mining.","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"genmax(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"maximal_itemsets/#RuleMiner.genmax-Tuple{Transactions, Union{Float64, Int64}}","page":"Maximal Itemset Mining","title":"RuleMiner.genmax","text":"genmax(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify maximal frequent itemsets in a transactional dataset with the GenMax algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nThe GenMax algorithm finds maximal frequent itemsets, which are frequent itemsets that are not  proper subsets of any other frequent itemset. It uses a depth-first search strategy with  pruning techniques like progressive focusing to discover these itemsets.\n\nThe algorithm proceeds in two main phases:\n\nCandidate Generation: Uses a depth-first search to generate candidate maximal frequent itemsets.\nMaximality Checking: Ensures that only truly maximal itemsets are retained in the final output.\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ' ')\n\n# Find maximal frequent itemsets with 5% minimum support\nresult = genmax(txns, 0.05)\n\n# Find maximal frequent itemsets with minimum 5,000 transactions\nresult = genmax(txns, 5_000)\n\nReferences\n\nGouda, Karam, and Mohammed J. Zaki. “GenMax: An Efficient Algorithm for Mining Maximal Frequent Itemsets.” Data Mining and Knowledge Discovery 11, no. 3 (November 1, 2005): 223–42. https://doi.org/10.1007/s10618-005-0002-x.\n\n\n\n\n\n","category":"method"},{"location":"transactions/#Transactions-Objects","page":"Transactions Objects","title":"Transactions Objects","text":"","category":"section"},{"location":"transactions/#Transactions","page":"Transactions Objects","title":"Transactions","text":"","category":"section"},{"location":"transactions/","page":"Transactions Objects","title":"Transactions Objects","text":"    Transactions","category":"page"},{"location":"transactions/#RuleMiner.Transactions","page":"Transactions Objects","title":"RuleMiner.Transactions","text":"Transactions\n\nA struct representing a collection of transactions in a sparse matrix format.\n\nFields\n\nmatrix::SparseMatrixCSC{Bool,Int64}: A sparse boolean matrix representing the transactions. Rows correspond to transactions, columns to items. A true value at position (i,j)  indicates that the item j is present in transaction i.\ncolkeys::Dict{Int,String}: A dictionary mapping column indices to item names. This allows retrieval of the original item names from the matrix column indices.\nlinekeys::Dict{Int,String}: A dictionary mapping row indices to transaction identifiers. This can be used to map matrix rows back to their original transaction IDs or line numbers.\n\nConstructors\n\nTransactions(matrix::SparseMatrixCSC{Bool,Int64}, colkeys::Dict{Int,String}, linekeys::Dict{Int,String})\n\nTransactions(df::DataFrame, indexcol::Union{Symbol,Nothing}=nothing)\n\nDescription\n\nThe Transactions struct provides an efficient representation of transaction data,  particularly useful for large datasets in market basket analysis, association rule mining, or similar applications where memory efficiency is crucial.\n\nThe sparse matrix representation allows for efficient storage and computation,  especially when dealing with datasets where each transaction contains only a small  subset of all possible items.\n\nDataFrame Constructor\n\nThe DataFrame constructor allows direct creation of a Transactions object from a DataFrame:\n\ndf: Input DataFrame where each row is a transaction and each column is an item.\nindexcol: Optional. Specifies a column to use as transaction identifiers.   If not provided, row numbers are used as identifiers.\n\nExamples\n\n# Create from existing data\nmatrix = SparseMatrixCSC{Bool,Int64}(...)\ncolkeys = Dict(1 => \"apple\", 2 => \"banana\", 3 => \"orange\")\nlinekeys = Dict(1 => \"T001\", 2 => \"T002\", 3 => \"T003\")\ntxns = Transactions(matrix, colkeys, linekeys)\n\n# Create from DataFrame\ndf = DataFrame(\n    ID = [\"T1\", \"T2\", \"T3\"],\n    Apple = [1, 0, 1],\n    Banana = [1, 1, 0],\n    Orange = [0, 1, 1]\n)\ntxns_from_df = Transactions(df, indexcol=:ID)\n\n# Access data\nitem_in_transaction = txns.matrix[2, 1]  # Check if item 1 is in transaction 2\nitem_name = txns.colkeys[1]              # Get the name of item 1\ntransaction_id = txns.linekeys[2]\n\n\n\n\n\n","category":"type"},{"location":"transactions/#load_transactions","page":"Transactions Objects","title":"load_transactions","text":"","category":"section"},{"location":"transactions/","page":"Transactions Objects","title":"Transactions Objects","text":"load_transactions(file::String, delimiter::Char; id_col::Bool = false, skiplines::Int = 0)","category":"page"},{"location":"transactions/#RuleMiner.load_transactions-Tuple{String, Char}","page":"Transactions Objects","title":"RuleMiner.load_transactions","text":"load_transactions(file::String, delimiter::Char; id_col::Bool = false, skiplines::Int = 0, nlines::Int = 0)::Transactions\n\nLoad transaction data from a file and return a Transactions struct.\n\nArguments\n\nfile::String: Path to the input file containing transaction data.\ndelimiter::Char: Character used to separate items in each transaction.\n\nKeyword Arguments\n\nid_col::Bool = false: If true, treats the first item in each line as a transaction identifier.\nskiplines::Int = 0: Number of lines to skip at the beginning of the file (e.g., for headers).\nnlines::Int = 0: Maximum number of lines to read. If 0, reads the entire file.\n\nReturns\n\nTransactions: A struct containing:\nmatrix: A sparse boolean matrix where rows represent transactions and columns represent items.\ncolkeys: A dictionary mapping column indices to item names.\nlinekeys: A dictionary mapping row indices to transaction identifiers.\n\nDescription\n\nThis function reads transaction data from a file, where each line represents a transaction and items are separated by the specified delimiter. It constructs a sparse matrix  representation of the transactions, with rows as transactions and columns as unique items.\n\nThe function uses memory mapping to read the file and construct the sparse matrix directly without materializing dense intermediate representations.\n\nNote\n\nThis function may not be suitable for extremely large files that exceed available system memory.\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ',', id_col=true, skiplines=1)\n\n\n\n\n\n","category":"method"},{"location":"transactions/#txns_to_df","page":"Transactions Objects","title":"txns_to_df","text":"","category":"section"},{"location":"transactions/","page":"Transactions Objects","title":"Transactions Objects","text":"txns_to_df(txns::Transactions; indexcol::Bool= false)","category":"page"},{"location":"transactions/#RuleMiner.txns_to_df-Tuple{Transactions}","page":"Transactions Objects","title":"RuleMiner.txns_to_df","text":"txns_to_df(txns::Transactions, id_col::Bool = false)::DataFrame\n\nConvert a Transactions object into a DataFrame.\n\nArguments\n\ntxns::Transactions: The Transactions object to be converted.\nid_col::Bool = false: (Optional) If true, includes an 'Index' column with transaction identifiers.\n\nReturns\n\nDataFrame: A DataFrame representation of the transactions.\n\nDescription\n\nThis function converts a Transactions object, which uses a sparse matrix representation, into a DataFrame. Each row of the resulting DataFrame represents a transaction, and each column represents an item.\n\nThe values in the DataFrame are integers, where 1 indicates the presence of an item in a transaction, and 0 indicates its absence.\n\nFeatures\n\nPreserves the original item names as column names.\nOptionally includes an 'Index' column with the original transaction identifiers.\n\nExample\n\n# Assuming 'txns' is a pre-existing Transactions object\ndf = txns_to_df(txns, id_col=true)\n\n\n\n\n\n","category":"method"},{"location":"closed_itemsets/#Closed-Itemset-Mining","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"(Image: Diagram showing maximal itemsets as a subset of closed itemsets which are a subset of frequent itemsets)","category":"page"},{"location":"closed_itemsets/#Description","page":"Closed Itemset Mining","title":"Description","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"Closed itemset mining is a set of techniques focused on discovering closed itemsets in a transactional dataset. A closed itemset is one which appears frequently in the data (above the minimum support threshold) and which has no superset with the same support. In other words, closed itemsets are the largest possible combinations of items that share the same transactions. They represent a lossless compression of the set of all frequent itemsets, as the support of any frequent itemset can be derived from the closed itemsets. ","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The key advantage of mining closed itemsets is that it provides a compact yet complete representation of all frequent patterns in the data. By identifying only the closed frequent itemsets, the number of patterns generated is significantly reduced compared to mining all frequent itemsets while still retaining all support information. This approach strikes a balance between the compactness of maximal itemsets and the completeness of all frequent itemsets. Closed itemset mining is particularly useful in scenarios where both the frequency and the exact composition of itemsets are important, but compression of the results is desired.","category":"page"},{"location":"closed_itemsets/#Formal-Definition","page":"Closed Itemset Mining","title":"Formal Definition","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"Let:","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"I be the set of all items in the dataset\nX be an itemset, where X subseteq I\nD be the set of all transactions in the dataset\nsigma(X) be the support of itemset X in D\nsigma_min be the minimum support threshold","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"Then, an itemset X is a closed frequent itemset if and only if:","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The support of X is greater than or equal to the minimum support threshold:","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"sigma(X) geq sigma_min","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"There does not exist a proper superset Y of X with the same support: ","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"nexists Y supset X  sigma(Y) = sigma(X)","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"Thus, CFI, the set of all closed frequent itemsets in I, can be expressed as:","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"CFI = X mid X subseteq I wedge sigma(X) geq sigma_min wedge nexists Y supset X  sigma(Y) = sigma(X)","category":"page"},{"location":"closed_itemsets/#Frequent-Itemset-Recovery","page":"Closed Itemset Mining","title":"Frequent Itemset Recovery","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"Closed itemsets can be used to recover all frequent itemsets by generating combinations from the mined itemsets along with their supports. This can be accomplished thorugh the levelwise algorithm proposed by Pasquier et al. in 1999.","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The levelwise function implements the levelwise algorithm for recovering frequent itemsets from closed itemsets. This algorithm generates all subsets of the closed itemsets, derives their supports, and then returns the results. This particular implementation is designed to take an output DataFrame from the various closed itemset mining algorithms in this package. Without the original transactions dataset, its input and return values can only handle absolute support (N), rather than both relative support and absolute support.","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"levelwise(df::DataFrame, min_n::Int)","category":"page"},{"location":"closed_itemsets/#RuleMiner.levelwise-Tuple{DataFrame, Int64}","page":"Closed Itemset Mining","title":"RuleMiner.levelwise","text":"levelwise(df::DataFrame, min_n::Int)::DataFrame\n\nRecover frequent itemsets from a DataFrame of closed itemsets\n\nArguments\n\ndf::DataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\nmin_support::Int: The minimum support threshold for the rules. This algorithm only takes absolute (integer) support\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ' ')\n\n# Find closed frequent itemsets with minimum 5,000 transactions\nclosed_sets = LCM(txns, 5_000)\n\n# Recover frequent itemsets from the closed itemsets\nfrequent_sets = levelwise(closed_sets, 5_000)\n\nReferences\n\nPasquier, Nicolas, Yves Bastide, Rafik Taouil, and Lotfi Lakhal. “Efficient Mining of Association Rules Using Closed Itemset Lattices.”  Information Systems 24, no. 1 (March 1, 1999): 25–46. https://doi.org/10.1016/S0306-4379(99)00003-4.\n\n\n\n\n\n","category":"method"},{"location":"closed_itemsets/#Algorithms","page":"Closed Itemset Mining","title":"Algorithms","text":"","category":"section"},{"location":"closed_itemsets/#CHARM","page":"Closed Itemset Mining","title":"CHARM","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The charm function implements the CHARM ([C]losed, [H]ash-based [A]ssociation [R]ule [M]ining) algorithm for mining closed itemsets proposed by Mohammad Zaki and Ching-Jui Hsiao in 2002. This algorithm uses a depth-first search with hash-based pruning approaches for non-closed itemsets and is particularly efficient for sparse datasets.","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"charm(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"closed_itemsets/#RuleMiner.charm-Tuple{Transactions, Union{Float64, Int64}}","page":"Closed Itemset Mining","title":"RuleMiner.charm","text":"charm(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify closed frequent itemsets in a transactional dataset with the CHARM algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nCHARM is an algorithm that builds on the ECLAT algorithm but adds additional closed-ness checking to return only closed itemsets. It uses a depth-first approach, exploring the search space and checking found itemsets against previously discovered itemsets to determine closedness.\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ' ')\n\n# Find closed frequent itemsets with 5% minimum support\nresult = charm(txns, 0.05)\n\n# Find closed frequent itemsets with minimum 5,000 transactions\nresult = charm(txns, 5_000)\n\nReferences\n\nZaki, Mohammed, and Ching-Jui Hsiao. “CHARM: An Efficient Algorithm for Closed Itemset Mining.” In Proceedings of the 2002 SIAM International Conference on Data Mining (SDM), 457–73. Proceedings. Society for Industrial and Applied Mathematics, 2002. https://doi.org/10.1137/1.9781611972726.27.\n\n\n\n\n\n","category":"method"},{"location":"closed_itemsets/#FPClose","page":"Closed Itemset Mining","title":"FPClose","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The fpclose function implements the FPClose ([F]requent [P]attern Close) algorithm for mining closed itemsets. This algorithm, proposed by Gösta Grahne and Jianfei Zhu in 2005, builds on the FP-Growth alogrithm to discover closed itemsets in a dataset without candidate generation. It inherits many of the advantages of FP-Growth when it comes to dense datasets.","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"fpclose(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"closed_itemsets/#RuleMiner.fpclose-Tuple{Transactions, Union{Float64, Int64}}","page":"Closed Itemset Mining","title":"RuleMiner.fpclose","text":"fpclose(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify closed frequent itemsets in a transactional dataset with the FPClose algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nThe FPClose algorithm is an extension of FP-Growth with  additional pruning techniques to focus on mining closed itemsets. The algorithm operates in two main phases:\n\nFP-tree Construction: Builds a compact representation of the dataset, organizing items  by their frequency to allow efficient mining.\nRecursive Tree Traversal: \nProcesses itemsets from least frequent to most frequent.\nFor each item, creates a conditional FP-tree and recursively mines it.\nUses a depth-first search strategy, exploring longer itemsets before shorter ones.\nEmploys pruning techniques to avoid generating non-closed itemsets.\n\nFPClose is particularly efficient for datasets with long transactions or sparse frequent itemsets,  as it can significantly reduce the number of generated itemsets compared to algorithms that  find all frequent itemsets.\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ' ')\n\n# Find closed frequent itemsets with 5% minimum support\nresult = fpclose(txns, 0.05)\n\n# Find closed frequent itemsets with minimum 5,000 transactions\nresult = fpclose(txns, 5_000)\n\nReferences\n\nGrahne, Gösta, and Jianfei Zhu. “Fast Algorithms for Frequent Itemset Mining Using FP-Trees.” IEEE Transactions on Knowledge and Data Engineering 17, no. 10 (October 2005): 1347–62. https://doi.org/10.1109/TKDE.2005.166.\n\n\n\n\n\n","category":"method"},{"location":"closed_itemsets/#LCM","page":"Closed Itemset Mining","title":"LCM","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The LCM function implements the LCM ([L]inear-time [C]losed [M]iner) algorithm for mining frequent closed itemsets first proposed by Uno et al. in 2004. This is an efficient method for discovering closed itemsets in a dataset with a linear time complexity. It is typically faster than other algorithms and has a more balance profile that achieves fast mining on both sparse and dense datasets.","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"   LCM(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"closed_itemsets/#RuleMiner.LCM-Tuple{Transactions, Union{Float64, Int64}}","page":"Closed Itemset Mining","title":"RuleMiner.LCM","text":"LCM(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify closed frequent itemsets in a transactional dataset with the LCM algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nLCM is an algorithm that uses a depth-first search pattern with closed-ness checking to return only closed itemsets. It utilizes two key pruning techniques to avoid redundant mining: prefix-preserving closure extension (PPCE) and progressive database reduction (PDR).\n\nPPCE ensures that each branch will never overlap in the itemsets they explore by enforcing the order of the itemsets. This reduces redunant search space.\nPDR works with PPCE to remove data from a branch's dataset once it is determined to be not nescessary.\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ' ')\n\n# Find closed frequent itemsets with 5% minimum support\nresult = LCM(txns, 0.05)\n\n# Find closed frequent itemsets with minimum 5,000 transactions\nresult = LCM(txns, 5_000)\n\nReferences\n\nUno, Takeaki, Tatsuya Asai, Yuzo Uchida, and Hiroki Arimura. “An Efficient Algorithm for Enumerating Closed Patterns in Transaction Databases.”  In Discovery Science, edited by Einoshin Suzuki and Setsuo Arikawa, 16–31. Berlin, Heidelberg: Springer, 2004. https://doi.org/10.1007/978-3-540-30214-8_2.\n\n\n\n\n\n","category":"method"},{"location":"closed_itemsets/#CARPENTER","page":"Closed Itemset Mining","title":"CARPENTER","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The carpenter function implements the CARPENTER ([C]losed [P]att[e]r[n] Discovery by [T]ransposing Tabl[e]s that a[r]e Extremely Long) algorithm for mining closed itemsets proposed by Pan et al. in 2003. This algorithm uses a transposed structure to optimize for datasets that have far more items than transactions, such as those found in genetic research and bioinformatics. It is not well suited to datasets in the more standard transaction-major format.","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"carpenter(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"closed_itemsets/#RuleMiner.carpenter-Tuple{Transactions, Union{Float64, Int64}}","page":"Closed Itemset Mining","title":"RuleMiner.carpenter","text":"carpenter(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify closed frequent itemsets in a transactional dataset with the CARPENTER algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nCARPENTER is an algorithm that progressively builds larger itemsets, checking closed-ness at each step with three key pruning strategies:\n\nItemsets are skipped if they have already been marked as closed on another branch\nItemsets are skipped if they do not meet minimum support\nItemsets' child itemsets are skipped if they change the support when the new items are added\n\nCARPENTER is specialized for datasets which have few transactions, but many items per transaction and may not be the best choice for other data.\n\nExample\n\ntxns = load_transactions(\"transactions.txt\", ' ')\n\n# Find closed frequent itemsets with 5% minimum support\nresult = carpenter(txns, 0.05)\n\n# Find closed frequent itemsets with minimum 5,000 transactions\nresult = carpenter(txns, 5_000)\n\nReferences\n\nPan, Feng, Gao Cong, Anthony K. H. Tung, Jiong Yang, and Mohammed J. Zaki. “Carpenter: Finding Closed Patterns in Long Biological Datasets.” In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 637–42. KDD ’03. New York, NY, USA: Association for Computing Machinery, 2003. https://doi.org/10.1145/956750.956832.\n\n\n\n\n\n","category":"method"},{"location":"#RuleMiner.jl","page":"Home","title":"RuleMiner.jl","text":"","category":"section"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RuleMiner.jl is a Julia package for association rule and frequent itemset mining inspired by the arules R package and SPMF Java library.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Key features of RuleMiner.jl include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Support for Julia's native multithreading capabilities for improved performance\nDirect interfaces with DataFrames.jl for loading transactional data and exporting results\nFlexible handling of either relative (percentage) support or absolute (count) support in minimum support thresholds","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"association_rules.md\", \"frequent_itemsets.md\", \"closed_itemsets.md\",\"maximal_itemsets.md\",\"transactions.md\"]\nDepth = 2","category":"page"}]
}
