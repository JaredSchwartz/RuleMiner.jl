var documenterSearchIndex = {"docs":
[{"location":"frequent_itemsets/#Frequent-Itemset-Mining","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"(Image: Diagram showing maximal itemsets as a subset of closed itemsets which are a subset of frequent itemsets)","category":"page"},{"location":"frequent_itemsets/#Description","page":"Frequent Itemset Mining","title":"Description","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"Frequent itemset mining is a fundamental technique in data mining focused on discovering itemsets that appear frequently in a transactional dataset. A frequent itemset is a set of items that occurs together in the data with a frequency no less than a specified minimum support threshold. Frequent itemsets form the basis of various data mining tasks, including association rule mining, sequential pattern mining, and correlation analysis.","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"The one caveat with frequent itemset mining is that depending on the support parameter and the structure of the data, these mining techniques can yield large numbers of patterns, especially in dense datasets or with low support thresholds. This challenge has led to the development of more concise representations like closed and maximal itemset mining.","category":"page"},{"location":"frequent_itemsets/#Formal-Definition","page":"Frequent Itemset Mining","title":"Formal Definition","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"Let:","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"I be the set of all items in the dataset\nX be an itemset, where X subseteq I\nD be the set of all transactions in the dataset\nsigma(X) be the support of itemset X in D\nsigma_min be the minimum support threshold","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"Then, an itemset X is a maximal frequent itemset if and only if: 1.\tThe support of X is greater than or equal to the minimum support threshold: ","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"sigma(X) geq sigma_min","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"Thus, FI, the set of all maximal frequent itemsets in I can be expressed as:","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"FI = X mid X subseteq I wedge sigma(X) geq sigma_min","category":"page"},{"location":"frequent_itemsets/#Algorithms","page":"Frequent Itemset Mining","title":"Algorithms","text":"","category":"section"},{"location":"frequent_itemsets/#ECLAT","page":"Frequent Itemset Mining","title":"ECLAT","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"The eclat function implements the [E]quivalence [CLA]ss [T]ransformation algorithm for frequent itemset mining proposed by Mohammad Zaki in 2000. This algorithm identifies frequent itemsets in a dataset utilizing a column-first search and supplied minimum support.","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"eclat(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"frequent_itemsets/#RuleMiner.eclat-Tuple{Transactions, Union{Float64, Int64}}","page":"Frequent Itemset Mining","title":"RuleMiner.eclat","text":"eclat(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nPerform frequent itemset mining using the ECLAT (Equivalence CLAss Transformation) algorithm  on a transactional dataset.\n\nECLAT is an efficient algorithm for discovering frequent itemsets, which are sets of items  that frequently occur together in the dataset.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nA DataFrame containing the discovered frequent itemsets with the following columns:\n\nItemset: Vector of item names in the frequent itemset.\nSupport: Relative support of the itemset.\nN: Absolute support count of the itemset.\nLength: Number of items in the itemset.\n\nAlgorithm Description\n\nThe ECLAT algorithm uses a depth-first search strategy and a vertical database layout to  efficiently mine frequent itemsets. It starts by computing the support of individual items,  sorts them in descending order of frequency, and then recursively builds larger itemsets. ECLAT's depth-first approach enables it to quickly identify long frequent itemsets, and it is most efficient for sparse datasets\n\nExample\n\ntxns = Txns(\"transactions.txt\", ' ')\n\n# Find frequent itemsets with 5% minimum support\nresult = eclat(txns, 0.05)\n\n# Find frequent itemsets with minimum 5,000 transactions\nresult = eclat(txns, 5_000)\n\nReferences\n\nZaki, Mohammed. “Scalable Algorithms for Association Mining.” Knowledge and Data Engineering, IEEE Transactions On 12 (June 1, 2000): 372–90. https://doi.org/10.1109/69.846291.\n\n\n\n\n\n","category":"method"},{"location":"frequent_itemsets/#FP-Growth","page":"Frequent Itemset Mining","title":"FP-Growth","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"The fpgrowth function implements the FP-Growth ([F]requent [P]attern Growth) algorithm for mining frequent itemsets. This algorithm, proposed by Han et al. in 2000, is an efficient method for discovering frequent itemsets in a dataset without candidate generation. It is generally more efficient than other algorithms when datasets are dense, as the internal FP tree data structure it builds efficiently summarizes the relationships and supports of the itemsets.","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"fpgrowth(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"frequent_itemsets/#RuleMiner.fpgrowth-Tuple{Transactions, Union{Float64, Int64}}","page":"Frequent Itemset Mining","title":"RuleMiner.fpgrowth","text":"fpgrowth(data::Union{Transactions,FPTree}, min_support::Union{Int,Float64})::DataFrame\n\nIdentify frequent itemsets in a transactional dataset or an FP-tree with the FPGrowth algorithm.\n\nArguments\n\ndata::Union{Transactions,FPTree}: Either a Transactions object containing the dataset to mine, or a pre-constructed FPTree object.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the frequent itemsets, with columns:\nItemset: The items in the frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nThe FPGrowth algorithm is a mining technique that builds a compact summary of the transaction  data called an FP-tree. This tree structure summarizes the supports and relationships between  items in a way that can be easily traversed and processed to find frequent itemsets.  FPGrowth is particularly efficient for datasets with long transactions or sparse frequent itemsets.\n\nThe algorithm operates in two main phases:\n\nFP-tree Construction: Builds a compact representation of the dataset, organizing items  by their frequency to allow efficient mining. This step is skipped if an FPTree is provided.\nRecursive Tree Traversal: \nProcesses itemsets from least frequent to most frequent.\nFor each item, creates a conditional FP-tree and recursively mines it.\n\nExample\n\n# Using a Transactions object\ntxns = Txns(\"transactions.txt\", ' ')\nresult = fpgrowth(txns, 0.05)  # Find frequent itemsets with 5% minimum support\n\n# Using a pre-constructed FPTree\ntree = FPTree(txns, 5000)  # Construct FP-tree with minimum support of 5000\nresult = fpgrowth(tree, 6000)  # Find frequent itemsets with minimum support of 6000\n\nReferences\n\nHan, Jiawei, Jian Pei, and Yiwen Yin. \"Mining Frequent Patterns without Candidate Generation.\"  SIGMOD Rec. 29, no. 2 (May 16, 2000): 1–12. https://doi.org/10.1145/335191.335372.\n\n\n\n\n\n","category":"method"},{"location":"frequent_itemsets/#Frequent-Itemset-Recovery","page":"Frequent Itemset Mining","title":"Frequent Itemset Recovery","text":"","category":"section"},{"location":"frequent_itemsets/#Recover-from-closed-itemsets","page":"Frequent Itemset Mining","title":"Recover from closed itemsets","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"Closed itemsets can be used to recover all frequent itemsets by generating combinations from the mined itemsets along with their supports. This can be accomplished thorugh the levelwise algorithm proposed by Pasquier et al. in 1999.","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"The recover_closed function implements the levelwise algorithm for recovering frequent itemsets from closed itemsets. This algorithm generates all subsets of the closed itemsets, derives their supports, and then returns the results. This particular implementation is designed to take an output DataFrame from the various closed itemset mining algorithms in this package. Without the original transactions dataset, its input and return values can only handle absolute support (N), rather than both relative support and absolute support.","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"recover_closed(df::DataFrame, min_n::Int)","category":"page"},{"location":"frequent_itemsets/#RuleMiner.recover_closed-Tuple{DataFrame, Int64}","page":"Frequent Itemset Mining","title":"RuleMiner.recover_closed","text":"recover_closed(df::DataFrame, min_n::Int)::DataFrame\n\nRecover frequent itemsets from a DataFrame of closed itemsets.\n\nArguments\n\ndf::DataFrame: A DataFrame containing the closed frequent itemsets, with columns:\nItemset: The items in the closed frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\nmin_n::Int: The minimum support threshold for the rules. This is the absolute (integer) support.\n\nReturns\n\nDataFrame: A DataFrame containing all frequent itemsets, with columns:\nItemset: The items in the frequent itemset.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nThis function recovers all frequent itemsets from a set of closed itemsets. It generates all possible subsets of the closed itemsets and calculates their supports based on the smallest containing closed itemset.\n\nThe function works as follows:\n\nIt filters the input DataFrame to only include closed sets above the minimum support.\nFor each length k from 1 to the maximum itemset length: a. It generates all k-subsets of the closed itemsets. b. For each subset, it finds the smallest closed itemset containing it. c. It assigns the support of the smallest containing closed itemset to the subset.\nIt combines all frequent itemsets and their supports into a result DataFrame.\n\nExample\n\ntxns = Txns(\"transactions.txt\", ' ')\n\n# Find closed frequent itemsets with minimum 5,000 transactions\nclosed_sets = fpclose(txns, 5_000)\n\n# Recover frequent itemsets from the closed itemsets\nfrequent_sets = recover_closed(closed_sets, 5_000)\n\nReferences\n\nPasquier, Nicolas, Yves Bastide, Rafik Taouil, and Lotfi Lakhal. \"Efficient Mining of Association Rules Using Closed Itemset Lattices.\" Information Systems 24, no. 1 (March 1, 1999): 25–46. https://doi.org/10.1016/S0306-4379(99)00003-4.\n\n\n\n\n\n","category":"method"},{"location":"frequent_itemsets/#Recover-from-maximal-itemsets","page":"Frequent Itemset Mining","title":"Recover from maximal itemsets","text":"","category":"section"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"Like closed itemsets, maximal itemsets can be used to recover all frequent itemsets, however unlike with closed itemsets, the supports of the itemsets cannot be recovered.","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"The recover_maximal performs this recovery by finding all subsets of the maximal itemsets and returns a DataFrame of the itemset and length.","category":"page"},{"location":"frequent_itemsets/","page":"Frequent Itemset Mining","title":"Frequent Itemset Mining","text":"recover_maximal(df::DataFrame)","category":"page"},{"location":"frequent_itemsets/#RuleMiner.recover_maximal-Tuple{DataFrame}","page":"Frequent Itemset Mining","title":"RuleMiner.recover_maximal","text":"recover_maximal(df::DataFrame)::DataFrame\n\nRecover all frequent itemsets from a DataFrame of maximal frequent itemsets.\n\nArguments\n\ndf::DataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nLength: The number of items in the itemset.\n\nReturns\n\nDataFrame: A DataFrame containing all frequent itemsets, with columns:\nItemset: The items in the frequent itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nThis function takes a DataFrame of maximal frequent itemsets and generates all possible subsets (including the maximal itemsets themselves) to recover the complete set of frequent itemsets. It does not calculate or recover support values, as these cannot be determined from maximal itemsets alone.\n\nThe function works as follows:\n\nFor each maximal itemset, it generates all possible subsets.\nIt combines all these subsets into a single collection of frequent itemsets.\nIt removes any duplicate itemsets that might arise from overlapping maximal itemsets.\nIt returns the result as a DataFrame, sorted by itemset length in descending order.\n\nExample\n\ntxns = Txns(\"transactions.txt\", ' ')\n\n# Find maximal frequent itemsets with minimum 5,000 transactions\nmaximal_sets = fpmax(txns, 5_000)\n\n# Recover frequent itemsets from the maximal itemsets\nfrequent_sets = recover_maximal(maximal_sets)\n\n\n\n\n\n","category":"method"},{"location":"association_rules/#Association-Rule-Mining","page":"Association Rule Mining","title":"Association Rule Mining","text":"","category":"section"},{"location":"association_rules/#Description","page":"Association Rule Mining","title":"Description","text":"","category":"section"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Association rule mining is a fundamental technique in data mining and machine learning that aims to uncover interesting relationships, correlations, or patterns within large datasets. Originally developed for market basket analysis in retail, it has since found applications in various fields such as web usage mining, intrusion detection, and bioinformatics. The primary goal of association rule mining is to identify strong association rules rules discovered in databases using different measures of interestingness like support, confidence, coverage, and lift.","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"At its core, association rule mining works by examining frequent if-then patterns in transactional databases. These patterns, known as association rules, take the form \"if A, then B,\" where A and B are sets of items. For example, in a supermarket context, a rule might be \"if a customer buys bread and butter, they are likely to buy milk.\" The strength of these rules is typically measured by support (how frequently the items appear together), confidence (how often the rule is found to be true), converage(how often B ocurrs in in the databse with or without A), and lift (the ratio of observed support to expected support if A and B were independent). By filtering association rules with these metrics, analysts can filter out weak or uninteresting rules and focus on those that are most likely to provide valuable insights or actionable information.","category":"page"},{"location":"association_rules/#Formal-Definition","page":"Association Rule Mining","title":"Formal Definition","text":"","category":"section"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Let:","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"I = i_1 i_2  i_n be the set of all items in the dataset\nD = T_1 T_2  T_m be the set of all transactions, where each T_j subseteq I\nA B subseteq I and A cap B = emptyset","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"An association rule is an implication of the form A Rightarrow B, where:","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"A is called the antecedent (or left-hand side)\nB is called the consequent (or right-hand side)","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"For a given rule A Rightarrow B, these measures are defined:","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Support: sigma(A Rightarrow B) = fracT_j in D  A cup B subseteq T_jD\nConfidence: chi(A Rightarrow B) = fracsigma(A cup B)sigma(A)\nCoverage: gamma(A Rightarrow B) = sigma(A) = fracT_j in D  A subseteq T_jD\nLift: L(A Rightarrow B) = fracsigma(A cup B)sigma(A) cdot sigma(B)","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Let sigma_min and chi_min be user-defined minimum thresholds for support and confidence, respectively.","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Then, the set of all valid association rules (AR) can be defined as:","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"AR = (A Rightarrow B) mid A B subseteq I newline\nwedge  A cap B = emptyset \nwedge sigma(A Rightarrow B) geq sigma_min \nwedge  chi(A Rightarrow B) geq chi_min","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"The process of association rule mining involves:","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"Finding all frequent itemsets F = Z subseteq I mid sigma(Z) geq sigma_min\nFor each frequent itemset Z in F, generate all non-empty subsets A subset Z\nFor each such subset A, form the rule A Rightarrow (Z setminus A) if chi(A Rightarrow (Z setminus A)) geq chi_min","category":"page"},{"location":"association_rules/#Algorithms","page":"Association Rule Mining","title":"Algorithms","text":"","category":"section"},{"location":"association_rules/#A-Priori","page":"Association Rule Mining","title":"A Priori","text":"","category":"section"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"The apriori function implements the A Priori algorithm for association rule mining first proposed by Rakesh Agrawal and Srikant Ramakrishnan in 1994. This algorithm identifies frequent itemsets in a dataset and generates association rules based on specified support thresholds.","category":"page"},{"location":"association_rules/","page":"Association Rule Mining","title":"Association Rule Mining","text":"apriori(txns::Transactions, min_support::Union{Int,Float64}, max_length::Int)","category":"page"},{"location":"association_rules/#RuleMiner.apriori-Tuple{Transactions, Union{Float64, Int64}, Int64}","page":"Association Rule Mining","title":"RuleMiner.apriori","text":"apriori(txns::Transactions, min_support::Union{Int,Float64}, max_length::Int)::DataFrame\n\nIdentify association rules in a transactional dataset using the A Priori Algorithm\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\nmax_length::Int: The maximum length of the rules to be generated.\n\nReturns\n\nA DataFrame containing the discovered association rules with the following columns:\n\nLHS: The left-hand side (antecedent) of the rule.\nRHS: The right-hand side (consequent) of the rule.\nSupport: Relative support of the rule.\nConfidence: Confidence of the rule.\nCoverage: Coverage (RHS support) of the rule.\nLift: Lift of the association rule.\nN: Absolute support of the association rule.\nLength: The number of items in the association rule.\n\nDescription\n\nThe Apriori algorithm employs a breadth-first, level-wise search strategy to discover  frequent itemsets. It starts by identifying frequent individual items and iteratively  builds larger itemsets by combining smaller frequent itemsets. At each iteration, it  generates candidate itemsets of size k from itemsets of size k-1, then prunes candidates  that have any infrequent subset. \n\nThe algorithm uses the downward closure property, which states that any subset of a frequent itemset must also be frequent. This is the defining pruning technique of A Priori. Once all frequent itemsets up to the specified maximum length are found, the algorithm generates association rules and  calculates their support, confidence, and other metrics.\n\nExample\n\ntxns = Txns(\"transactions.txt\", ' ')\n\n# Find rules with 5% min support and max length of 3\nresult = apriori(txns, 0.05, 3)\n\n# Find rules with with at least 5,000 instances and max length of 3\nresult = apriori(txns, 5_000, 3)\n\nReferences\n\nAgrawal, Rakesh, and Ramakrishnan Srikant. “Fast Algorithms for Mining Association Rules in Large Databases.” In Proceedings of the 20th International Conference on Very Large Data Bases, 487–99. VLDB ’94. San Francisco, CA, USA: Morgan Kaufmann Publishers Inc., 1994.\n\n\n\n\n\n","category":"method"},{"location":"fptree/#FPTree-Objects","page":"FP Tree Objects","title":"FPTree Objects","text":"","category":"section"},{"location":"fptree/#FPTree","page":"FP Tree Objects","title":"FPTree","text":"","category":"section"},{"location":"fptree/","page":"FP Tree Objects","title":"FP Tree Objects","text":"    FPTree","category":"page"},{"location":"fptree/#RuleMiner.FPTree","page":"FP Tree Objects","title":"RuleMiner.FPTree","text":"FPTree\n\nA struct representing an FP-Tree (Frequent Pattern Tree) structure, used for efficient frequent itemset mining.\n\nFields\n\nroot::FPNode: The root node of the FP-Tree.\nheader_table::Dict{Int, Vector{FPNode}}: A dictionary where keys are item indices and values are vectors of FPNodes representing the item occurrences in the tree.\ncol_mapping::Dict{Int, Int}: A dictionary mapping the condensed item indices to the original item indices.\nmin_support::Int: The minimum support threshold used to construct the tree.\nn_transactions::Int: The total number of transactions used to build the tree.\ncolkeys::Vector{String}: The original item names corresponding to the column indices.\n\nDescription\n\nThe FP-Tree is a compact representation of transaction data, designed for efficient frequent pattern mining.  It stores frequent items in a tree structure, with shared prefixes allowing for memory-efficient storage and fast traversal.\n\nThe tree construction process involves:\n\nCounting item frequencies and filtering out infrequent items.\nSorting items by frequency.\nInserting transactions into the tree, with items ordered by their frequency.\n\nThe header_table provides quick access to all occurrences of an item in the tree, facilitating efficient mining operations.\n\nConstructors\n\nDefault Constructor\n\nFPTree()\n\nTransaction Constructor\n\nFPTree(txns::Transactions, min_support::Union{Int,Float64})\n\nThe Transaction constructor allows creation of a FPTree object from a Transactions-type object:\n\ntxns: Transactions object to convert\nmin_support: Minimum support for an item to be included int the tree\n\nExamples\n\n# Create an empty FP-Tree\nempty_tree = FPTree()\n\n# Create an FP-Tree from a Transactions object\ntxns = Txns(\"transactions.txt\", ' ')\ntree = FPTree(txns, 0.05)  # Using 5% minimum support\n\n# Access tree properties\nprintln(\"Minimum support: \", tree.min_support)\nprintln(\"Number of transactions: \", tree.n_transactions)\nprintln(\"Number of unique items: \", length(tree.header_table))\n\n# Traverse the tree (example)\nfunction traverse(node::FPNode, prefix::Vector{String}=String[])\n    if node.value != -1\n        println(join(vcat(prefix, tree.colkeys[node.value]), \" -> \"))\n    end\n    for child in values(node.children)\n        traverse(child, vcat(prefix, node.value != -1 ? [tree.colkeys[node.value]] : String[]))\n    end\nend\n\ntraverse(tree.root)\n\nNotes\n\nThe FP-Tree structure is particularly useful for algorithms like FP-Growth, FP-Close, and FP-Max.\nWhen constructing from a Transactions object, items not meeting the minimum support threshold are excluded from the tree.\nThe tree construction process is parallelized for efficiency on multi-core systems.\n\nReferences\n\nHan, J., Pei, J., & Yin, Y. (2000). Mining Frequent Patterns without Candidate Generation.  In proceedings of the 2000 ACM SIGMOD International Conference on Management of Data (pp. 1-12).\n\n\n\n\n\n","category":"type"},{"location":"fptree/#FPNode","page":"FP Tree Objects","title":"FPNode","text":"","category":"section"},{"location":"fptree/","page":"FP Tree Objects","title":"FP Tree Objects","text":"    FPNode","category":"page"},{"location":"fptree/#RuleMiner.FPNode","page":"FP Tree Objects","title":"RuleMiner.FPNode","text":"FPNode\n\nA mutable struct representing a node in an FP-tree (Frequent Pattern Tree) structure.\n\nFields\n\nvalue::Int: The item index this node represents. For the root node, this is typically -1.\nsupport::Int: The number of transactions that contain this item in the path from the root to this node.\nchildren::Dict{Int, FPNode}: A dictionary of child nodes, where keys are item indices and values are FPNode objects.\nparent::Union{FPNode, Nothing}: The parent node in the FP-tree. For the root node, this is nothing.\n\nDescription\n\nFPNode is the fundamental building block of an FP-tree. Each node represents an item in the dataset  and keeps track of how many transactions contain the path from the root to this item. The tree structure  allows for efficient mining of frequent patterns without repeated database scans.\n\nThe children dictionary allows for quick access to child nodes, facilitating efficient tree traversal. The parent reference enables bottom-up traversal, which is crucial for some frequent pattern mining algorithms.\n\nConstructor\n\nFPNode(value::Int, parent::Union{FPNode, Nothing}=nothing)\n\nExamples\n\n# Create a root node\nroot = FPNode(-1)\n\n# Create child nodes\nchild1 = FPNode(1, root)\nchild2 = FPNode(2, root)\n\n# Add children to the root\nroot.children[1] = child1\nroot.children[2] = child2\n\n# Increase support of a node\nchild1.support += 1\n\n# Create a grandchild node\ngrandchild = FPNode(3, child1)\nchild1.children[3] = grandchild\n\n# Traverse the tree\nfunction print_tree(node::FPNode, depth::Int = 0)\n    println(\" \"^depth, \"Item: \", node.value, \", Support: \", node.support)\n    for child in values(node.children)\n        print_tree(child, depth + 2)\n    end\nend\n\nprint_tree(root)\n\n\n\n\n\n","category":"type"},{"location":"maximal_itemsets/#Maximal-Itemset-Mining","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"(Image: Diagram showing maximal itemsets as a subset of closed itemsets which are a subset of frequent itemsets)","category":"page"},{"location":"maximal_itemsets/#Description","page":"Maximal Itemset Mining","title":"Description","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"Maximal itemset mining is a set of techniques focused on discovering maximal itemsets in a transactional dataset. A maximal itemset is one which appears frequently in the data (above the minimum support threshold) and which is not a subset of any other frequent itemset. ","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"In other words, maximal itemsets are the largest possible combinations in the dataset of the items that meet a specified frequency threshold. They are a subset of closed itemsets, which in turn are a subset of all frequent itemsets.","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"The key advantage of mining maximal itemsets is its compact representation of all frequent patterns in the data. By identifying only the maximal frequent itemsets, the number of patterns generated is significantly reduced compared to frequent itemset mining. This approach is particularly valuable when dealing with high-dimensional data or datasets with long transactions.","category":"page"},{"location":"maximal_itemsets/#Formal-Definition","page":"Maximal Itemset Mining","title":"Formal Definition","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"Let:","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"I be the set of all items in the dataset\nX be an itemset, where X subseteq I\nD be the set of all transactions in the dataset\nsigma(X) be the support of itemset X in D\nsigma_min be the minimum support threshold","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"Then, an itemset X is a maximal frequent itemset if and only if: 1.\tThe support of X is greater than or equal to the minimum support threshold: ","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"sigma(X) geq sigma_min","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"2.\tThere does not exist a superset Y of X such that Y is also frequent: ","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"nexists Y supset X  sigma(Y) geq sigma_min","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"Thus, MFI, the set of all maximal frequent itemsets in I can be expressed as:","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"MFI = X mid X subseteq I wedge sigma(X) geq sigma_min wedge nexists Y supset X  sigma(Y) geq sigma_min","category":"page"},{"location":"maximal_itemsets/#Frequent-Itemset-Recovery","page":"Maximal Itemset Mining","title":"Frequent Itemset Recovery","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"Maximal itemsets can be used to recover all frequent itemsets by generating combinations from the mined itemset. However, unlike with closed itemsets, recovering the support of the frequent combinations is not possible.","category":"page"},{"location":"maximal_itemsets/#Algorithms","page":"Maximal Itemset Mining","title":"Algorithms","text":"","category":"section"},{"location":"maximal_itemsets/#FPMax","page":"Maximal Itemset Mining","title":"FPMax","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"The fpmax function implements the FPMax ([F]requent [P]attern Max) algorithm for mining closed itemsets. This algorithm, proposed by Gösta Grahne and Jianfei Zhu in 2005, builds on the FP-Growth alogrithm by mining FP trees to discover maximal itemsets in a dataset. It inherits many of the advantages of FP-Growth when it comes to dense datasets.","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"fpmax(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"maximal_itemsets/#RuleMiner.fpmax-Tuple{Transactions, Union{Float64, Int64}}","page":"Maximal Itemset Mining","title":"RuleMiner.fpmax","text":"fpmax(data::Union{Transactions,FPTree}, min_support::Union{Int,Float64})::DataFrame\n\nIdentify maximal frequent itemsets in a transactional dataset or an FP-tree with the FPMax algorithm.\n\nArguments\n\ndata::Union{Transactions,FPTree}: Either a Transactions object containing the dataset to mine, or a pre-constructed FPTree object.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nThe FPMax algorithm is an extension of FP-Growth with additional pruning techniques  to focus on mining maximal itemsets. The algorithm operates in three main phases:\n\nFP-tree Construction: Builds a compact representation of the dataset, organizing items  by their frequency to allow efficient mining. This step is skipped if an FPTree is provided.\nRecursive Tree Traversal: \nProcesses itemsets from least frequent to most frequent.\nFor each item, creates a conditional FP-tree and recursively mines it.\nUses a depth-first search strategy, exploring longer itemsets before shorter ones.\nEmploys pruning techniques to avoid generating non-maximal itemsets.\nAdds an itemset to the candidate set when no frequent superset exists.\nMaximality Checking: After the recursive traversal, filters the candidate set to ensure  only truly maximal itemsets are included in the final output.\n\nFPMax is particularly efficient for datasets with long transactions or sparse frequent itemsets,  as it can significantly reduce the number of generated itemsets compared to algorithms that  find all frequent itemsets.\n\nExample\n\n# Using a Transactions object\ntxns = Txns(\"transactions.txt\", ' ')\nresult = fpmax(txns, 0.05)  # Find maximal frequent itemsets with 5% minimum support\n\n# Using a pre-constructed FPTree\ntree = FPTree(txns, 5000)  # Construct FP-tree with minimum support of 5000\nresult = fpmax(tree, 6000)  # Find maximal frequent itemsets with minimum support of 6000\n\nReferences\n\nGrahne, Gösta, and Jianfei Zhu. \"Fast Algorithms for Frequent Itemset Mining Using FP-Trees.\"  IEEE Transactions on Knowledge and Data Engineering 17, no. 10 (October 2005): 1347–62.  https://doi.org/10.1109/TKDE.2005.166.\n\n\n\n\n\n","category":"method"},{"location":"maximal_itemsets/#GenMax","page":"Maximal Itemset Mining","title":"GenMax","text":"","category":"section"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"The genmax function implements the GenMax algorithm for mining closed itemsets. This algorithm, proposed by Karam Gouda and Mohammad Zaki in 2005, utilizes a technique called progressive focusing to reduce the search space for maximal itemset mining.","category":"page"},{"location":"maximal_itemsets/","page":"Maximal Itemset Mining","title":"Maximal Itemset Mining","text":"genmax(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"maximal_itemsets/#RuleMiner.genmax-Tuple{Transactions, Union{Float64, Int64}}","page":"Maximal Itemset Mining","title":"RuleMiner.genmax","text":"genmax(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify maximal frequent itemsets in a transactional dataset with the GenMax algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nThe GenMax algorithm finds maximal frequent itemsets, which are frequent itemsets that are not  proper subsets of any other frequent itemset. It uses a depth-first search strategy with  pruning techniques like progressive focusing to discover these itemsets.\n\nThe algorithm proceeds in two main phases:\n\nCandidate Generation: Uses a depth-first search to generate candidate maximal frequent itemsets.\nMaximality Checking: Ensures that only truly maximal itemsets are retained in the final output.\n\nExample\n\ntxns = Txns(\"transactions.txt\", ' ')\n\n# Find maximal frequent itemsets with 5% minimum support\nresult = genmax(txns, 0.05)\n\n# Find maximal frequent itemsets with minimum 5,000 transactions\nresult = genmax(txns, 5_000)\n\nReferences\n\nGouda, Karam, and Mohammed J. Zaki. “GenMax: An Efficient Algorithm for Mining Maximal Frequent Itemsets.” Data Mining and Knowledge Discovery 11, no. 3 (November 1, 2005): 223–42. https://doi.org/10.1007/s10618-005-0002-x.\n\n\n\n\n\n","category":"method"},{"location":"transactions/#Transactions-Objects","page":"Transactions Objects","title":"Transactions Objects","text":"","category":"section"},{"location":"transactions/#Txns-(Frequent-Transactions)","page":"Transactions Objects","title":"Txns (Frequent Transactions)","text":"","category":"section"},{"location":"transactions/","page":"Transactions Objects","title":"Transactions Objects","text":"    Txns","category":"page"},{"location":"transactions/#RuleMiner.Txns","page":"Transactions Objects","title":"RuleMiner.Txns","text":"Txns <: Transactions\n\nA struct representing a collection of transactions in a sparse matrix format.\n\nFields\n\nmatrix::SparseMatrixCSC{Bool,Int64}: A sparse boolean matrix representing the transactions. Rows correspond to transactions, columns to items. A true value at position (i,j)  indicates that the item j is present in transaction i.\ncolkeys::Vector{String}: A vector of item names corresponding to matrix columns.\nlinekeys::Vector{String}: A vector of transaction identifiers corresponding to matrix rows.\nn_transactions::Int: The total number of transactions in the dataset.\n\nDescription\n\nThe Txns struct provides an efficient representation of transaction data,  particularly useful for large datasets in market basket analysis, association rule mining, or similar applications where memory efficiency is crucial.\n\nThe sparse matrix representation allows for efficient storage and computation,  especially when dealing with datasets where each transaction contains only a small  subset of all possible items.\n\nConstructors\n\nDefault Constructor\n\nTxns(matrix::SparseMatrixCSC{Bool,Int64}, colkeys::Vector{String}, linekeys::Vector{String})\n\nDataFrame Constructor\n\nTxns(df::DataFrame, indexcol::Union{Symbol,Nothing}=nothing)\n\nThe DataFrame constructor allows direct creation of a Txns object from a DataFrame:\n\ndf: Input DataFrame where each row is a transaction and each column is an item.\nindexcol: Optional. Specifies a column to use as transaction identifiers.   If not provided, row numbers are used as identifiers.\n\nFile Constructor\n\nTxns(file::String, delimiter::Union{Char,String}; id_col::Bool = false, skiplines::Int = 0, nlines::Int = 0)\n\nThe file constructor allows creation of a Txns object directly from a file:\n\nfile: Path to the input file containing transaction data.\ndelimiter: Character or string used to separate items in each transaction.\n\nKeyword Arguments:\n\nid_col: If true, treats the first item in each line as a transaction identifier.\nskiplines: Number of lines to skip at the beginning of the file (e.g., for headers).\nnlines: Maximum number of lines to read. If 0, reads the entire file.\n\nExamples\n\n# Create from existing data\nmatrix = SparseMatrixCSC{Bool,Int64}(...)\ncolkeys = [\"apple\", \"banana\", \"orange\"]\nlinekeys = [\"T001\", \"T002\", \"T003\"]\ntxns = Txns(matrix, colkeys, linekeys)\n\n# Create from DataFrame\ndf = DataFrame(\n    ID = [\"T1\", \"T2\", \"T3\"],\n    Apple = [1, 0, 1],\n    Banana = [1, 1, 0],\n    Orange = [0, 1, 1]\n)\ntxns_from_df = Txns(df, indexcol=:ID)\n\n# Create from file with character delimiter\ntxns_from_file_char = Txns(\"transactions.txt\", ',', id_col=true, skiplines=1)\n\n# Create from file with string delimiter\ntxns_from_file_string = Txns(\"transactions.txt\", \"||\", id_col=true, skiplines=1)\n\n# Access data\nitem_in_transaction = txns.matrix[2, 1]  # Check if item 1 is in transaction 2\nitem_name = txns.colkeys[1]              # Get the name of item 1\ntransaction_id = txns.linekeys[2]        # Get the ID of transaction 2\ntotal_transactions = txns.n_transactions # Get the total number of transactions\n\n\n\n\n\n","category":"type"},{"location":"transactions/","page":"Transactions Objects","title":"Transactions Objects","text":"txns_to_df(txns::Txns)","category":"page"},{"location":"transactions/#RuleMiner.txns_to_df-Tuple{Txns}","page":"Transactions Objects","title":"RuleMiner.txns_to_df","text":"txns_to_df(txns::Txns, id_col::Bool = false)::DataFrame\n\nConvert a Txns object into a DataFrame.\n\nArguments\n\ntxns::Txns: The Txns object to be converted.\n\nReturns\n\nDataFrame: A DataFrame representation of the transactions.\n\nDescription\n\nThis function converts a Txns object, which uses a sparse matrix representation, into a DataFrame. Each row of the resulting DataFrame represents a transaction, and each column represents an item.\n\nThe values in the DataFrame are integers, where 1 indicates the presence of an item in a transaction, and 0 indicates its absence.\n\nFeatures\n\nPreserves the original item names as column names.\nOptionally includes an 'Index' column with the original transaction identifiers.\n\nExample\n\n# Assuming 'txns' is a pre-existing Txns object\ndf = txns_to_df(txns, id_col=true)\n\n\n\n\n\n","category":"method"},{"location":"transactions/#SeqTxns-(Sequential-Transactions)","page":"Transactions Objects","title":"SeqTxns (Sequential Transactions)","text":"","category":"section"},{"location":"transactions/","page":"Transactions Objects","title":"Transactions Objects","text":"    SeqTxns","category":"page"},{"location":"transactions/#RuleMiner.SeqTxns","page":"Transactions Objects","title":"RuleMiner.SeqTxns","text":"SeqTxns <: Transactions\n\nA struct representing a collection of transactions in a sparse matrix format, with support for sequence grouping.\n\nFields\n\nmatrix::SparseMatrixCSC{Bool,Int64}: A sparse boolean matrix representing the transactions. Rows correspond to transactions, columns to items. A true value at position (i,j)  indicates that the item j is present in transaction i.\ncolkeys::Vector{String}: A vector of item names corresponding to matrix columns.\nlinekeys::Vector{String}: A vector of transaction identifiers corresponding to matrix rows.\nindex::Vector{UInt32}: A vector of indices indicating the start of each new sequence. The last sequence ends at the last row of the matrix.\nn_transactions::Int: The total number of transactions in the dataset.\n\nDescription\n\nThe SeqTxns struct extends the concept of transaction data to include sequence information. It provides an efficient representation for datasets where transactions are grouped into sequences, such as time-series data or grouped purchasing behaviors. This structure is particularly useful for sequential pattern mining and other sequence-aware data mining tasks.\n\nThe sparse matrix representation allows for efficient storage and computation,  especially when dealing with datasets where each transaction contains only a small  subset of all possible items.\n\nConstructors\n\nDefault Constructor\n\nSeqTxns(matrix::SparseMatrixCSC{Bool,Int64}, colkeys::Vector{String}, linekeys::Vector{String}, index::Vector{UInt32})\n\nDataFrame Constructor\n\nSeqTxns(df::DataFrame, sequence_col::Symbol, index_col::Union{Symbol,Nothing}=nothing)\n\nThe DataFrame constructor allows direct creation of a SeqTxns object from a DataFrame:\n\ndf: Input DataFrame where each row is a transaction and each column is an item.\nsequence_col: Specifies the column used to determine sequence groupings.\nindex_col: Optional. Specifies a column to use as transaction identifiers.   If not provided, row numbers are used as identifiers.\n\nFile Constructor\n\nSeqTxns(file::String, item_delimiter::Union{Char,String}, set_delimiter::Union{Char,String}; id_col::Bool = false, skiplines::Int = 0, nlines::Int = 0)\n\nThe file constructor allows creation of a SeqTxns object directly from a file:\n\nfile: Path to the input file containing transaction data.\nitem_delimiter: Character or string used to separate items within a transaction.\nset_delimiter: Character or string used to separate transactions within a sequence.\n\nKeyword Arguments:\n\nid_col: If true, treats the first item in each transaction as a transaction identifier.\nskiplines: Number of lines to skip at the beginning of the file (e.g., for headers).\nnlines: Maximum number of lines to read. If 0, reads the entire file.\n\nExamples\n\n# Create from DataFrame\ndf = DataFrame(\n    ID = [\"T1\", \"T2\", \"T3\", \"T4\", \"T5\", \"T6\"],\n    Sequence = [\"A\", \"A\", \"B\", \"B\", \"B\", \"C\"],\n    Apple = [1, 0, 1, 0, 1, 1],\n    Banana = [1, 1, 0, 1, 0, 1],\n    Orange = [0, 1, 1, 1, 0, 0]\n)\ntxns_seq = SeqTxns(df, :Sequence, index_col=:ID)\n\n# Create from file\ntxns_seq_file = SeqTxns(\"transactions.txt\", ',', ';', id_col=true, skiplines=1)\n\n# Access data\nitem_in_transaction = txns_seq.matrix[2, 1]  # Check if item 1 is in transaction 2\nitem_name = txns_seq.colkeys[1]              # Get the name of item 1\ntransaction_id = txns_seq.linekeys[2]        # Get the ID of transaction 2\nsequence_starts = txns_seq.index             # Get the starting indices of each sequence\ntotal_transactions = txns.n_transactions # Get the total number of transactions\n\n# Get bounds of a specific sequence (e.g., second sequence)\nseq_start = txns_seq.index[2]\nseq_end = txns_seq.index[3] - 1  # Or length(txns_seq.linekeys) if it's the last sequence\n\n\n\n\n\n","category":"type"},{"location":"transactions/","page":"Transactions Objects","title":"Transactions Objects","text":"    txns_to_df(txns::SeqTxns, index::Bool = true)","category":"page"},{"location":"transactions/#RuleMiner.txns_to_df","page":"Transactions Objects","title":"RuleMiner.txns_to_df","text":"txns_to_df(txns::SeqTxns, id_col::Bool = false)::DataFrame\n\nConvert a SeqTxns object into a DataFrame, including sequence information.\n\nArguments\n\ntxns::SeqTxns: The SeqTxns object to be converted.\nid_col::Bool = false: If true, includes an 'Index' column with transaction identifiers.\n\nReturns\n\nDataFrame: A DataFrame representation of the transactions with the following columns:\nItem columns: One column for each item, with 1 indicating presence and 0 indicating absence.\n'SequenceIndex': A column indicating which sequence each transaction belongs to.\n\nDescription\n\nThis function converts a SeqTxns object, which uses a sparse matrix representation with sequence  information, into a DataFrame. Each row of the resulting DataFrame represents a transaction,  each column represents an item, and an additional column represents the sequence index.\n\nThe values in the item columns are integers, where 1 indicates the presence of an item in a transaction, and 0 indicates its absence.\n\nThe 'SequenceIndex' column contains an integer for each row, indicating which sequence the  transaction belongs to. Sequences are numbered starting from 1.\n\nExample\n\n# Assuming 'txns_seq' is a pre-existing SeqTxns object\ndf = txns_to_df(txns_seq, id_col=true)\n\n\n\n\n\n\n","category":"function"},{"location":"closed_itemsets/#Closed-Itemset-Mining","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"(Image: Diagram showing maximal itemsets as a subset of closed itemsets which are a subset of frequent itemsets)","category":"page"},{"location":"closed_itemsets/#Description","page":"Closed Itemset Mining","title":"Description","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"Closed itemset mining is a set of techniques focused on discovering closed itemsets in a transactional dataset. A closed itemset is one which appears frequently in the data (above the minimum support threshold) and which has no superset with the same support. In other words, closed itemsets are the largest possible combinations of items that share the same transactions. They represent a lossless compression of the set of all frequent itemsets, as the support of any frequent itemset can be derived from the closed itemsets. ","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The key advantage of mining closed itemsets is that it provides a compact yet complete representation of all frequent patterns in the data. By identifying only the closed frequent itemsets, the number of patterns generated is significantly reduced compared to mining all frequent itemsets while still retaining all support information. This approach strikes a balance between the compactness of maximal itemsets and the completeness of all frequent itemsets. Closed itemset mining is particularly useful in scenarios where both the frequency and the exact composition of itemsets are important, but compression of the results is desired.","category":"page"},{"location":"closed_itemsets/#Formal-Definition","page":"Closed Itemset Mining","title":"Formal Definition","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"Let:","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"I be the set of all items in the dataset\nX be an itemset, where X subseteq I\nD be the set of all transactions in the dataset\nsigma(X) be the support of itemset X in D\nsigma_min be the minimum support threshold","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"Then, an itemset X is a closed frequent itemset if and only if:","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The support of X is greater than or equal to the minimum support threshold:","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"sigma(X) geq sigma_min","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"There does not exist a proper superset Y of X with the same support: ","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"nexists Y supset X  sigma(Y) = sigma(X)","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"Thus, CFI, the set of all closed frequent itemsets in I, can be expressed as:","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"CFI = X mid X subseteq I wedge sigma(X) geq sigma_min wedge nexists Y supset X  sigma(Y) = sigma(X)","category":"page"},{"location":"closed_itemsets/#Algorithms","page":"Closed Itemset Mining","title":"Algorithms","text":"","category":"section"},{"location":"closed_itemsets/#CHARM","page":"Closed Itemset Mining","title":"CHARM","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The charm function implements the CHARM ([C]losed, [H]ash-based [A]ssociation [R]ule [M]ining) algorithm for mining closed itemsets proposed by Mohammad Zaki and Ching-Jui Hsiao in 2002. This algorithm uses a depth-first search with hash-based pruning approaches for non-closed itemsets and is particularly efficient for sparse datasets.","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"charm(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"closed_itemsets/#RuleMiner.charm-Tuple{Transactions, Union{Float64, Int64}}","page":"Closed Itemset Mining","title":"RuleMiner.charm","text":"charm(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify closed frequent itemsets in a transactional dataset with the CHARM algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nCHARM is an algorithm that builds on the ECLAT algorithm but adds additional closed-ness checking to return only closed itemsets. It uses a depth-first approach, exploring the search space and checking found itemsets against previously discovered itemsets to determine closedness.\n\nExample\n\ntxns = Txns(\"transactions.txt\", ' ')\n\n# Find closed frequent itemsets with 5% minimum support\nresult = charm(txns, 0.05)\n\n# Find closed frequent itemsets with minimum 5,000 transactions\nresult = charm(txns, 5_000)\n\nReferences\n\nZaki, Mohammed, and Ching-Jui Hsiao. “CHARM: An Efficient Algorithm for Closed Itemset Mining.” In Proceedings of the 2002 SIAM International Conference on Data Mining (SDM), 457–73. Proceedings. Society for Industrial and Applied Mathematics, 2002. https://doi.org/10.1137/1.9781611972726.27.\n\n\n\n\n\n","category":"method"},{"location":"closed_itemsets/#FPClose","page":"Closed Itemset Mining","title":"FPClose","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The fpclose function implements the FPClose ([F]requent [P]attern Close) algorithm for mining closed itemsets. This algorithm, proposed by Gösta Grahne and Jianfei Zhu in 2005, builds on the FP-Growth alogrithm to discover closed itemsets in a dataset without candidate generation. It inherits many of the advantages of FP-Growth when it comes to dense datasets.","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"fpclose(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"closed_itemsets/#RuleMiner.fpclose-Tuple{Transactions, Union{Float64, Int64}}","page":"Closed Itemset Mining","title":"RuleMiner.fpclose","text":"fpclose(data::Union{Transactions,FPTree}, min_support::Union{Int,Float64})::DataFrame\n\nIdentify closed frequent itemsets in a transactional dataset or an FP-tree with the FPClose algorithm.\n\nArguments\n\ndata::Union{Transactions,FPTree}: Either a Transactions object containing the dataset to mine, or a pre-constructed FPTree object.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the closed frequent itemsets, with columns:\nItemset: The items in the closed frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nThe FPClose algorithm is an extension of FP-Growth with additional pruning techniques  to focus on mining closed itemsets. The algorithm operates in two main phases:\n\nFP-tree Construction: Builds a compact representation of the dataset, organizing items  by their frequency to allow efficient mining. This step is skipped if an FPTree is provided.\nRecursive Tree Traversal: \nProcesses itemsets from least frequent to most frequent.\nFor each item, creates a conditional FP-tree and recursively mines it.\nUses a depth-first search strategy, exploring longer itemsets before shorter ones.\nEmploys pruning techniques to avoid generating non-closed itemsets.\n\nFPClose is particularly efficient for datasets with long transactions or sparse frequent itemsets,  as it can significantly reduce the number of generated itemsets compared to algorithms that  find all frequent itemsets.\n\nExample\n\n# Using a Transactions object\ntxns = Txns(\"transactions.txt\", ' ')\nresult = fpclose(txns, 0.05)  # Find closed frequent itemsets with 5% minimum support\n\n# Using a pre-constructed FPTree\ntree = FPTree(txns, 5000)  # Construct FP-tree with minimum support of 5000\nresult = fpclose(tree, 6000)  # Find closed frequent itemsets with minimum support of 6000\n\nReferences\n\nGrahne, Gösta, and Jianfei Zhu. \"Fast Algorithms for Frequent Itemset Mining Using FP-Trees.\"  IEEE Transactions on Knowledge and Data Engineering 17, no. 10 (October 2005): 1347–62.  https://doi.org/10.1109/TKDE.2005.166.\n\n\n\n\n\n","category":"method"},{"location":"closed_itemsets/#LCM","page":"Closed Itemset Mining","title":"LCM","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The LCM function implements the LCM ([L]inear-time [C]losed [M]iner) algorithm for mining frequent closed itemsets first proposed by Uno et al. in 2004. This is an efficient method for discovering closed itemsets in a dataset with a linear time complexity. It is typically faster than other algorithms and has a more balance profile that achieves fast mining on both sparse and dense datasets.","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"   LCM(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"closed_itemsets/#RuleMiner.LCM-Tuple{Transactions, Union{Float64, Int64}}","page":"Closed Itemset Mining","title":"RuleMiner.LCM","text":"LCM(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify closed frequent itemsets in a transactional dataset with the LCM algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nLCM is an algorithm that uses a depth-first search pattern with closed-ness checking to return only closed itemsets. It utilizes two key pruning techniques to avoid redundant mining: prefix-preserving closure extension (PPCE) and progressive database reduction (PDR).\n\nPPCE ensures that each branch will never overlap in the itemsets they explore by enforcing the order of the itemsets. This reduces redunant search space.\nPDR works with PPCE to remove data from a branch's dataset once it is determined to be not nescessary.\n\nExample\n\ntxns = Txns(\"transactions.txt\", ' ')\n\n# Find closed frequent itemsets with 5% minimum support\nresult = LCM(txns, 0.05)\n\n# Find closed frequent itemsets with minimum 5,000 transactions\nresult = LCM(txns, 5_000)\n\nReferences\n\nUno, Takeaki, Tatsuya Asai, Yuzo Uchida, and Hiroki Arimura. “An Efficient Algorithm for Enumerating Closed Patterns in Transaction Databases.”  In Discovery Science, edited by Einoshin Suzuki and Setsuo Arikawa, 16–31. Berlin, Heidelberg: Springer, 2004. https://doi.org/10.1007/978-3-540-30214-8_2.\n\n\n\n\n\n","category":"method"},{"location":"closed_itemsets/#CARPENTER","page":"Closed Itemset Mining","title":"CARPENTER","text":"","category":"section"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"The carpenter function implements the CARPENTER ([C]losed [P]att[e]r[n] Discovery by [T]ransposing Tabl[e]s that a[r]e Extremely Long) algorithm for mining closed itemsets proposed by Pan et al. in 2003. This algorithm uses a transposed structure to optimize for datasets that have far more items than transactions, such as those found in genetic research and bioinformatics. It is not well suited to datasets in the more standard transaction-major format.","category":"page"},{"location":"closed_itemsets/","page":"Closed Itemset Mining","title":"Closed Itemset Mining","text":"carpenter(txns::Transactions, min_support::Union{Int,Float64})","category":"page"},{"location":"closed_itemsets/#RuleMiner.carpenter-Tuple{Transactions, Union{Float64, Int64}}","page":"Closed Itemset Mining","title":"RuleMiner.carpenter","text":"carpenter(txns::Transactions, min_support::Union{Int,Float64})::DataFrame\n\nIdentify closed frequent itemsets in a transactional dataset with the CARPENTER algorithm.\n\nArguments\n\ntxns::Transactions: A Transactions object containing the dataset to mine.\nmin_support::Union{Int,Float64}: The minimum support threshold. If an Int, it represents  the absolute support. If a Float64, it represents relative support.\n\nReturns\n\nDataFrame: A DataFrame containing the maximal frequent itemsets, with columns:\nItemset: The items in the maximal frequent itemset.\nSupport: The relative support of the itemset as a proportion of total transactions.\nN: The absolute support count of the itemset.\nLength: The number of items in the itemset.\n\nDescription\n\nCARPENTER is an algorithm that progressively builds larger itemsets, checking closed-ness at each step with three key pruning strategies:\n\nItemsets are skipped if they have already been marked as closed on another branch\nItemsets are skipped if they do not meet minimum support\nItemsets' child itemsets are skipped if they change the support when the new items are added\n\nCARPENTER is specialized for datasets which have few transactions, but many items per transaction and may not be the best choice for other data.\n\nExample\n\ntxns = Txns(\"transactions.txt\", ' ')\n\n# Find closed frequent itemsets with 5% minimum support\nresult = carpenter(txns, 0.05)\n\n# Find closed frequent itemsets with minimum 5,000 transactions\nresult = carpenter(txns, 5_000)\n\nReferences\n\nPan, Feng, Gao Cong, Anthony K. H. Tung, Jiong Yang, and Mohammed J. Zaki. “Carpenter: Finding Closed Patterns in Long Biological Datasets.” In Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 637–42. KDD ’03. New York, NY, USA: Association for Computing Machinery, 2003. https://doi.org/10.1145/956750.956832.\n\n\n\n\n\n","category":"method"},{"location":"#RuleMiner.jl","page":"Home","title":"RuleMiner.jl","text":"","category":"section"},{"location":"#Introduction","page":"Home","title":"Introduction","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"RuleMiner.jl is a Julia package for association rule and frequent itemset mining inspired by the arules R package and SPMF Java library.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Key features of RuleMiner.jl include:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Support for Julia's native multithreading capabilities for improved performance\nDirect interfaces with DataFrames.jl for loading transactional data and exporting results\nFlexible handling of either relative (percentage) support or absolute (count) support in minimum support thresholds","category":"page"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pages = [\"association_rules.md\", \"frequent_itemsets.md\", \"closed_itemsets.md\",\"maximal_itemsets.md\",\"transactions.md\"]\nDepth = 2","category":"page"}]
}
